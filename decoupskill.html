<!DOCTYPE html>

<html class="no-js" lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Decoupling Skill Learning from Robotic Control for Generalizable Manipulation</title>
	<link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
	<link rel="icon" href="favicon.ico" type="image/x-icon">
    <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,900" rel="stylesheet">
    <link rel="stylesheet" href="libs/font-awesome/css/font-awesome.min.css">
    <!--??-->
    <link href="css/project.css" rel="stylesheet">
</head>

<body>

    <div id="main" style="padding-bottom:1em; padding-top: 5em; width: 60em; max-width: 80em; margin-left: auto; margin-right: auto;">
        <section id="four">

            <h1 style="text-align: center; margin-bottom: 0;">
                Decoupling Skill Learning from Robotic Control for Generalizable Object Manipulation
            </h1>
            <br>
            <section>
                <div class="box alt" style="margin-bottom: 1em;">
                    <h5 style="text-align: center;">      
                            Kai Lu<sup>1</sup>, 
                            Bo Yang<sup>2</sup>, 
                            Bing Wang<sup>1</sup>,
                            Andrew Markham<sup>1</sup>
                </div>
            </section>




            <h6 style="color: #a2a2a2; margin-bottom: 2em;"><sup>1</sup> K. Lu, B. Wang, and A. Markham are with the Department 
                of Computer Science, University of Oxford, Oxford, UK. {kai.lu, bing.wang, andrew.markham}@cs.ox.ac.uk<br>
                <sup>2</sup>  B. Yang is with vLAR Group, Department of Computing, Hong Kong Polytechnic University, HKSAR. bo.yang@polyu.edu.hk<br>
            </h6>




            <h4 style="text-align: center">
                <a href="https://arxiv.org/abs/2303.04016" style="text-decoration: underline; text-underline-offset: 5px;">paper link</a>
                <!-- &nbsp; &nbsp; &nbsp;
                <a href="https://todo">code</a> -->
            </h4>

            
            <div class="12u$">
                <video class="image fit" style="margin-bottom: 0.5em;" controls="" muted autoplay="autoplay" loop="loop">
                    <source src="images/RobotManiSkill/gen2multi.mov" type="video/mp4">Your browser does not support this video.
                </video>
            </div>

            <hr>



            <b><h2 style="text-align: center;">Summary Video (ICRA 2023)</h2></b>
            
            <div class="12u$">
                <video class="image fit" style="margin-bottom: 0.5em;" controls="" data-video="0">
                    <source src="images/RobotManiSkill/icra_cm_kaivoi_18MB.mp4" type="video/mp4">Your browser does not support this video.
                </video>
            </div>

            <hr>

            <!-- <br> -->
            <b><h2 style="text-align: center;">Abstract</h2></b>
            <p>
                Robotic reinforcement learning (RL) and imitation learning (IL) have recently shown potential for tackling a range of tasks 
                e.g., opening a drawer or a cupboard, but they generalize poorly to unseen objects. 
                In this paper, we separate the task of learning 'what to do' from 'how to do it' i.e., whole-body control (WBC). 
                We pose the RL problem as one of determining the skill dynamics for a disembodied virtual manipulator interacting with articulated 
                objects (<em><b>left panel</b></em>).
                The QP-based WBC is optimized with singularity and kinematic constraints to execute the high-dimensional joint motion to reach the 
                goals in the workspace (<em><b>right panel</b></em>). 
                Experiments on manipulating complex articulated objects show that our approach is more 
                generalizable to unseen objects with large intra-class variations. It also
                generates more compliant robotic motion, and outperforms the pure RL and IL 
                baselines in task success rates (<em><b>bottom panel</b></em>). <a href="#exp">quick view of example results (click)</a>
            </p>

            <center>
                <div class="9u"><a href=""><span class="image fit"><img src="images/RobotManiSkill/decouple_core_icrav32.png" alt="" align="top" style="margin-top: -0px;"></span></a></div>
            </center>

            <hr>
            <b><h2 style="text-align: center;">Pipeline</h2></b>
            <p>
                As shown in the yellow block, we use two simple PointNet ensembles to separately perceive 
                static (e.g., size of an object) and dynamic (e.g., current position of a handle) states. 
                These are inputs to a SAC RL framework to learn how to control the disembodied end-effector, 
                realizing a 6-DoF motion skill. Through knowledge of the robot's physical model, QP is used 
                to optimize control of the joint dynamics of the whole-body robot.
            </p>

            <center>
                <div class="10u"><a href=""><span class="image fit"><img src="images/RobotManiSkill/pipeline_icrav2.png" alt="" align="top" style="margin-top: -0px;"></span></a></div>
            </center>



            <hr>

            <b><h2 style="text-align: center;">Training process</h2></b>

            <div class="box alt">
                <p> During training process (<em><b>left</b></em>), the disembodied manipulator interactes with various cabinets. 
                    Test on unseen cabinets (<em><b>right</b></em>): by interacting with more cabinets, the RL model shows a better understanding of the skill dynamics, 
                    resulting in smoother and more reasonable motions.
                </p>
                <div class="row 50% uniform">
                    <div class="6u"><video class="image fit" style="margin-bottom: 0.5em;" controls="" data-video="1">
                        <source src="images/RobotManiSkill/hand_learn_train_concat3.mp4" type="video/mp4">Your browser does not support this video.</video>
                        <!-- <h5 style="color: #a2a2a2; margin-bottom: 1em;">Total # of actions: 7 (task complete)</h5> -->
                    </div>
                    <div class="6u$"><video class="image fit" style="margin-bottom: 0.5em;" controls="" data-video="2">
                        <source src="images/RobotManiSkill/hand_learn_test_concat.mp4" type="video/mp4">Your browser does not support this video.</video>
                        <!-- <h5 style="color: #a2a2a2; margin-bottom: 1em;">Total # of actions: 7 (task complete)</h5> -->
                    </div>            
                </div>
            </div>


            <hr>

            <b><h2 style="text-align: center;">Whole-body control</h2></b>

            <p> At every time step, the ego-centric point cloud observation is obtained from the three RGB-D cameras mounted on the robot.
             </p>
            <div class="11u"><a href=""><span class="image fit"><img src="images/RobotManiSkill/frame.png" alt=""></span></a></div>


            <!-- <center>
                <div class="10u"><a href=""><span class="image fit"><img src="images/RobotManiSkill/controller.png" alt=""></span></a></div>
            </center> -->
            <p>
                During the manipulation process, we optimize the joint-space actions of the 
                robot to approximate its end-effector (EE) motions to the disembodied manipulator's trajectory. 
                We use a QP-based WBC with robotic singularity and kinematic contraints to solve the high-dimensional joint actions.
               </p>
            <div class="12u$">
                <video class="image fit" style="margin-bottom: 0.5em;" controls="" data-video="0">
                    <source src="images/RobotManiSkill/syn2.mov" type="video/mp4">Your browser does not support this video.
                </video>
            </div>



            <hr>
            <b><h2 id="exp" style="text-align: center;">Example Results</h2></b>
            <h3>Generalizability to different unseen objects</h3>
            <p> Experiments show that our approach can learn generalizable skills over different cabinets of the training sets and unseen test sets. 
                We achieve an average success rate of <b>74% on training cabinets and a 51% on test cabinets</b> in the drawer opening task, significantly 
                out-performing existing techniques (e.g., the baseline methods in ManiSkill-Learn<sup><a href="https://github.com/haosulab/ManiSkill-Learn" style="text-decoration:none" >1</a></sup>
                  obtain a best performance 
                of <b>37% on training cabinets and a 12% on test cabinets</b>). 
            </p>

            <div class="box alt">
                <div class="row 50% uniform">
                    <div class="6u"><video class="image fit" style="margin-bottom: 0.5em;" controls="" data-video="3">
                        <source src="images/RobotManiSkill/hand_rob_1024.mp4" type="video/mp4">Your browser does not support this video.</video>
                        <!-- <h5 style="color: #a2a2a2; margin-bottom: 1em;">Total # of actions: 7 (task complete)</h5> -->
                    </div>
                    <div class="6u$"><video class="image fit" style="margin-bottom: 0.5em;" controls="" data-video="4">
                        <source src="images/RobotManiSkill/hand_rob_1038.mp4" type="video/mp4">Your browser does not support this video.</video>
                        <!-- <h5 style="color: #a2a2a2; margin-bottom: 1em;">Total # of actions: 7 (task complete)</h5> -->
                    </div>
                    <div class="6u"><video class="image fit" style="margin-bottom: 0.5em;" controls="" data-video="5">
                        <source src="images/RobotManiSkill/hand_rob_1045.mp4" type="video/mp4">Your browser does not support this video.</video>
                        <!-- <h5 style="color: #a2a2a2; margin-bottom: 1em;">Total # of actions: 7 (task complete)</h5> -->
                    </div>
                    <div class="6u$"><video class="image fit" style="margin-bottom: 0.5em;" controls="" data-video="6">
                        <source src="images/RobotManiSkill/hand_rob_1063_fix.mp4" type="video/mp4">Your browser does not support this video.</video>
                        <!-- <h5 style="color: #a2a2a2; margin-bottom: 1em;">Total # of actions: 7 (task complete)</h5> -->
                    </div>  
                </div>
            </div>

            <!-- <h6 style="color: #a2a2a2; margin-bottom: 2em;">Note:</h6> -->

            <h3>Motion Compliance</h3>
            <p> We also compare the robotic motions produced by our method and pure RL and IL (<em><b>left: BC in 0:00~0:12, BCQ in 0:13~0:25</b></em>), 
                showing that robot singularities are avoided in most cases by our method (<em><b>right: ours</b></em>). Note that BC and BCQ exhibits far high
                joint velocities, while our approach generates <b>more compliant, smoother, and controllable robot motions</b>.
            </p>

            <div class="box alt">
                <div class="row 50% uniform">
                    <div class="6u"><video class="image fit" style="margin-bottom: 0.5em;" controls="" data-video="1">
                        <source src="images/RobotManiSkill/bc_bcq_demo.mp4" type="video/mp4">Your browser does not support this video.</video>
                        <!-- <h5 style="color: #a2a2a2; margin-bottom: 1em;">Total # of actions: 7 (task complete)</h5> -->
                    </div>
                    <div class="6u$"><video class="image fit" style="margin-bottom: 0.5em;" controls="" data-video="2">
                        <source src="images/RobotManiSkill/ours_demo_fix.mp4" type="video/mp4">Your browser does not support this video.</video>
                        <!-- <h5 style="color: #a2a2a2; margin-bottom: 1em;">Total # of actions: 7 (task complete)</h5> -->
                    </div>            
                </div>
            </div>

            <hr>
            <b><h3>Contact</h3></b>
            <p>Have any questions, please feel free to contact <a href="https://DeerKK.github.io/">Kai Lu</a></p>
            <hr>

            <div class="row">
                <div class="6u 12u$(xsmall)">
                    <p>March, 2023<br>
                        Copyright &copy; <a href="https://DeerKK.github.io/">Kai Lu</a>
                    </p>
                </div>
            </div>
        </section>
    </div>
<!-- 
    <script src="js/project/main.js"></script>
    <script src="js/project/util.js"></script>
    <script src="js/project/skel.min.js"></script>
    <script src="js/project/jquery.min.js"></script>
    <script src="js/project/jquery.poptrox.min.js"></script> -->
</body>

</html>
